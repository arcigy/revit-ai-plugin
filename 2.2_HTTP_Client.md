# 2.2 HTTP Client

## üéØ Purpose
The HTTP Client is responsible for **sending structured requests** from the backend to the AI LLM service and **receiving responses**.  
It acts as the **communication layer** between the backend Request Builder and the AI Orchestrator.

---

## üß± Component Structure

| Component | Description | Type |
|-----------|------------|------|
| `http_client.py` | Main HTTP client class for AI communication | Python Class |
| `models.py` | Pydantic models for request/response validation | Python |
| `exceptions.py` | Custom exceptions for request errors | Python |

---

## üß≠ Workflow Overview

[RequestBuilder.build_payload()]
‚îÇ
‚ñº
[HTTPClient.send_request(payload)]
‚îÇ
‚ñº
[LLM API Response]
‚îÇ
‚ñº
[HTTPClient.parse_response()]
‚îÇ
‚ñº
[Structured object returned to AI Orchestrator]

python


---

## üß© Functionality

### 1. Sending Requests

- Sends HTTP POST requests to AI backend (OpenAI, Claude, or internal LLM).  
- Uses `async` for non-blocking execution.  
- Includes headers for authentication and content type.  
- Handles timeouts and retries.

**Python Example:**
```python
import httpx
import json

class HTTPClient:
    def __init__(self, base_url: str, api_key: str, timeout: int = 30):
        self.base_url = base_url
        self.api_key = api_key
        self.timeout = timeout

    async def send_request(self, endpoint: str, payload: dict) -> dict:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                response = await client.post(f"{self.base_url}/{endpoint}", json=payload, headers=headers)
                response.raise_for_status()
                return response.json()
            except httpx.RequestError as e:
                raise ConnectionError(f"Request failed: {e}") from e
            except httpx.HTTPStatusError as e:
                raise RuntimeError(f"Invalid response [{e.response.status_code}]: {e.response.text}") from e
2. Response Parsing
Parses JSON returned from AI into structured CommandResponse objects.

Validates that workflow JSON includes:

steps (list of actions)

Each action has command and parameters

Python Example:

python

from models import CommandResponse, Workflow, ActionStep

def parse_response(response_json: dict) -> CommandResponse:
    try:
        steps = [ActionStep(command=step["command"], parameters=step["parameters"])
                 for step in response_json["workflow"]["steps"]]
        workflow = Workflow(steps=steps)
        errors = response_json.get("errors", [])
        return CommandResponse(workflow=workflow, errors=errors)
    except KeyError as e:
        raise ValueError(f"Missing expected key in response: {e}")
3. Error Handling
Network errors ‚Üí ConnectionError

Invalid JSON ‚Üí ValueError

API errors ‚Üí RuntimeError

Ensures AI Orchestrator receives clear, structured information about success or failure.

Python Example:

python

try:
    response_json = await http_client.send_request("interpret", payload)
    command_response = parse_response(response_json)
except Exception as e:
    command_response = CommandResponse(workflow=None, errors=[str(e)])
4. Logging
Optional logging for debugging:

Request payload

Response payload

Time taken

Errors

Python Example:

python

import logging
logging.basicConfig(level=logging.INFO)

logging.info(f"Request payload: {json.dumps(payload)}")
logging.info(f"Response: {json.dumps(response_json)}")
‚ö†Ô∏è Key Considerations
Timeouts & Retries: AI services may respond slowly; implement exponential backoff if needed.

Async Safety: Use async/await to avoid blocking backend event loop.

Validation: Always parse and validate AI responses before sending to Execution Engine.

Security: Never log API keys or sensitive user data.

Extensibility: Should easily switch between multiple AI endpoints (OpenAI, Claude, local LLM).

üì¶ Suggested File Structure

/Backend
  ‚îú‚îÄ‚îÄ http_client.py
  ‚îú‚îÄ‚îÄ models.py
  ‚îî‚îÄ‚îÄ exceptions.py
‚úÖ Objective
After implementing the HTTP Client:

Backend can reliably send requests to AI LLMs.

Responses are parsed into structured workflow objects.

Network errors, invalid responses, or missing fields are handled gracefully.

Ready to integrate with AI Orchestrator for end-to-end workflow execution